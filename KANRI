
https://www.dropbox.com/scl/fi/lrjyypqrliq03v4wc7cif/binance_BTCUSDT_dollar_20170801-20250312_threshold36000000.csv?rlkey=28mbutrtaco2f216bi0oogsin&st=4dhkw64c&dl=0

def get_sparse_ind_matrix(samples_info_sets, price_bars):
    """
    Creates a sparse indicator matrix where rows are timestamps and columns are label indices.
    This is an optimized version from MlFinLab 1.5.0 that improves performance by up to 3000x.
    
    :param samples_info_sets: (pd.Series): Triple barrier events(t1) from labeling.get_events
    :param price_bars: (pd.DataFrame): Price bars which were used to form triple barrier events
    :return: (tuple): (sparse_ind_mat, denominators)
        - sparse_ind_mat: scipy.sparse.csr_matrix with labels as the rows and timestamps as columns
        - denominators: numpy array of denominators for calculating uniqueness
    """
    from scipy import sparse
    import pandas as pd
    import numpy as np
    
    if bool(samples_info_sets.isnull().values.any()) is True or bool(
            samples_info_sets.index.isnull().any()) is True:
        raise ValueError('NaN values in triple_barrier_events, delete nans')
    
    triple_barrier_events = pd.DataFrame(samples_info_sets)  # Convert Series to DataFrame
    
    # Create a dictionary to map timestamps to their position in the array
    trimmed_price_bars_index = price_bars[(price_bars.index >= triple_barrier_events.index.min()) &
                                         (price_bars.index <= triple_barrier_events.t1.max())].index
    
    # Create a unique sorted list of all timestamps
    bar_index = list(triple_barrier_events.index)  # Start indices
    bar_index.extend(triple_barrier_events.t1)     # End indices
    bar_index.extend(trimmed_price_bars_index)     # Price bar indices
    bar_index = sorted(list(set(bar_index)))       # Unique sorted timestamps
    
    # Map timestamps to positions
    timestamp_to_position = {timestamp: position for position, timestamp in enumerate(bar_index)}
    
    # Prepare data for sparse matrix construction
    rows = []
    cols = []
    data = []
    
    # For each label, find start and end positions, then populate sparse matrix data
    for sample_idx, (start_time, end_time) in enumerate(
            zip(triple_barrier_events.index, triple_barrier_events.t1)):
        start_pos = timestamp_to_position[start_time]
        end_pos = timestamp_to_position[end_time]
        
        # Add entries for all positions between start and end (inclusive)
        for pos in range(start_pos, end_pos + 1):
            rows.append(sample_idx)  # Label index
            cols.append(pos)         # Timestamp position
            data.append(1.0)         # Value (always 1)
    
    # Create sparse matrix (transposed compared to the dense version - labels as rows)
    num_labels = len(triple_barrier_events)
    num_timestamps = len(bar_index)
    sparse_ind_mat = sparse.csr_matrix((data, (rows, cols)), shape=(num_labels, num_timestamps))
    
    # Calculate denominators (concurrency) for uniqueness computation
    denominators = np.zeros(num_timestamps)
    for col_idx in range(num_timestamps):
        col_data = sparse_ind_mat.getcol(col_idx).data
        denominators[col_idx] = col_data.sum()
    
    return sparse_ind_mat, denominators


def sparse_seq_bootstrap(sparse_ind_mat, denominators, sample_length=None, warmup_samples=None, 
                         verbose=False, random_state=None):
    """
    Sparse matrix implementation of sequential bootstrapping that is up to 3000x faster
    than the original implementation.
    
    :param sparse_ind_mat: (scipy.sparse.csr_matrix) Sparse indicator matrix from get_sparse_ind_matrix
    :param denominators: (np.array) Denominators for calculating uniqueness
    :param sample_length: (int) Length of bootstrapped sample
    :param warmup_samples: (list) List of previously drawn samples
    :param verbose: (boolean) Flag to print updated probabilities on each step
    :param random_state: (np.random.RandomState) Random state
    :return: (array) Bootstrapped samples indexes
    """
    import numpy as np
    from scipy import sparse
    
    if random_state is None:
        random_state = np.random.RandomState()
    
    if sample_length is None:
        sample_length = sparse_ind_mat.shape[0]
    
    if warmup_samples is None:
        warmup_samples = []
    
    phi = []  # Bootstrapped samples
    # Initialize a sparse matrix for accumulated concurrency
    prev_concurrency = np.zeros(sparse_ind_mat.shape[1])
    
    while len(phi) < sample_length:
        # Calculate average uniqueness for each label
        avg_unique = np.zeros(sparse_ind_mat.shape[0])
        
        for label_idx in range(sparse_ind_mat.shape[0]):
            # Get the non-zero elements of this label
            row = sparse_ind_mat.getrow(label_idx)
            indices = row.indices
            
            if indices.size > 0:
                # Calculate uniqueness for each timestamp where this label has a value
                uniqueness_values = []
                for idx in indices:
                    if denominators[idx] + prev_concurrency[idx] > 0:  # Avoid division by zero
                        uniqueness = 1.0 / (denominators[idx] + prev_concurrency[idx])
                        uniqueness_values.append(uniqueness)
                
                if uniqueness_values:
                    avg_unique[label_idx] = np.mean(uniqueness_values)
        
        # Normalize to get probabilities
        if np.sum(avg_unique) > 0:
            prob = avg_unique / np.sum(avg_unique)
        else:
            # If all uniqueness values are zero, use uniform probability
            prob = np.ones(sparse_ind_mat.shape[0]) / sparse_ind_mat.shape[0]
        
        # Choose the next sample
        try:
            choice = warmup_samples.pop(0)
        except IndexError:
            choice = random_state.choice(range(sparse_ind_mat.shape[0]), p=prob)
        
        phi.append(choice)
        
        # Update accumulated concurrency
        row = sparse_ind_mat.getrow(choice)
        indices = row.indices
        for idx in indices:
            prev_concurrency[idx] += 1
        
        if verbose:
            print(prob)
    
    return phi
can you give me an overview of how demands for semiconductor components like  ICF, ICG, NW fluctuate? what are the drivers?


The demand for semiconductor components like ICF, ICG, and NW fluctuates due to various factors, including technological advancements, economic conditions, and geopolitical events. Here's an overview of the demand drivers for these components:

ICF (Integrated Circuit Foundry):

Technological advancements: The continuous miniaturization of transistors and the development of new chip architectures drive the demand for foundry services.
5G and AI: The increasing adoption of 5G technology and artificial intelligence requires high-performance chips, which boosts demand for foundry services.   
Automotive industry: The rise of autonomous vehicles and electric vehicles necessitates advanced semiconductor components, leading to increased demand for foundry services.   
IoT and edge computing: The proliferation of IoT devices and edge computing requires energy-efficient and low-cost chips, which also drives demand for foundry services.   
ICG (Integrated Circuit Packaging):

Miniaturization: The need for smaller and more compact electronic devices drives the demand for advanced packaging technologies.   
High-performance computing: High-performance computing applications require efficient heat dissipation and signal integrity, which can be achieved through advanced packaging solutions.   
Heterogeneous integration: The integration of different types of chips (e.g., CPUs, GPUs, memory) on a single package increases the demand for advanced packaging technologies.   
NW (Nanowire):

Electronics: Nanowires have the potential to revolutionize electronics by enabling smaller and more energy-efficient devices.   
Sensors: Nanowires can be used to develop highly sensitive sensors for various applications, such as environmental monitoring and medical diagnostics.   
Energy storage: Nanowires can be used to create high-capacity batteries and supercapacitors.   
In addition to these factors, geopolitical events like trade tensions and supply chain disruptions can also impact the demand for semiconductor components. For example, the US-China trade war has led to increased demand for semiconductor manufacturing capacity in regions outside of China.   

Overall, the demand for semiconductor components like ICF, ICG, and NW is driven by a combination of technological advancements, emerging applications, and global economic trends. As technology continues to evolve, the demand for these components is expected to remain strong.



https://skforecast.org/0.9.1/user_guides/autoregresive-forecaster
https://gemini.google.com/app/4892006fefd77094
https://skforecast.org/0.7.0/api/forecasterautoreg


it seems that exogenous variables are natively supported here?
https://skforecast.org/0.7.0/api/forecasterautoreg

You're absolutely right! I apologize for the previous misinformation.

According to the documentation you linked (https://skforecast.org/0.4.0/guides/autoregresive-forecaster-exogenous), exogenous variables are indeed natively supported by ForecasterAutoreg in skforecast.

The class provides mechanisms to incorporate these external features into your forecasting model, potentially improving its accuracy.

I provided an example in the previous response that outlined how to create a custom predictor function to handle exogenous variables. However, it seems there might be a built-in way to handle them through the exogenous argument in the ForecasterAutoreg constructor, as suggested by the documentation.

It's always recommended to refer to the official documentation for the most up-to-date information on library functionalities.



!!!CAPEX => https://chatgpt.com/share/671337d2-1cb4-8000-882c-874e7f28046a

https://www.displaysupplychain.co.jp/report/quarterly-display-fab-utilization-report


https://gemini.google.com/app/86252a91209def5c
2. Global Semiconductor Market Dynamics:
Wafer Fab Capacity: The total capacity of semiconductor manufacturing facilities worldwide.
Fab Utilization Rates: The percentage of capacity being used by fabs.
Capital Expenditure (Capex): The amount of money invested by semiconductor companies in new equipment and facilities.


USE LSTM!!!


====================================




https://blog.hoxo-m.com/entry/2020/02/20/090000

https://colab.research.google.com/drive/1u8dfeWGyUgdAnbp2JuPyvqOfvgXE6Mmf#scrollTo=fnr3bYyxy56w

https://chatgpt.com/share/3b75ed74-d39d-4ee1-a427-f89a286539a5

***
https://www.imes.boj.or.jp/jp/conference/finance/2022_slides/1111finws_slide2.pdf
SHAPによる機械学習モデルは「データに語らせている分析」であるため、メカニズムの解 釈や因果関係の検証には、別途、因果推論等の手法を用いる必要である。ただし、構造推定 のための予備的分析や変動要因の把握などで、機械学習モデルは有用なツールとなりえる
Esg breakdown or theme score breakdown.
予測の要因分解と時間変化の同時展開
企業ごと

https://www.google.com/imgres?imgurl=https://dol.ismcdn.jp/mwimgs/f/3/650/img_f3b0a638fca06637bce3addb0a9122c6387094.jpg&tbnid=WoNxMYm_moE5jM&vet=1&imgrefurl=https://diamond.jp/articles/-/307306?page%3D2&docid=GNbi7gk4bXKfcM&w=650&h=645&source=sh/x/im/m1/2&kgs=b891679317187246&shem=abme,trie


***
⭐️みずほスコアの向上が目的というよりかは、分析のためのツールの位置付け
マテリアリティの特定は 経営戦略と一体

マテリアリティ 重要課題= と始めるページに書いておいて その後はずっとマテリアリティ
マテリアリティは 対談つまり 定性的に決められたことが多いのでそれを定量的に見直す プロセスが必要

SASB、IIRCのマテ項目チェック overlap with mizuho?

エントロピーのイメージ図作れるか 社会 取締役

シナジー＝Esにどう取り組むかを左右するのがｇ

Eda first step
- PCA using corr エントロピーベース like 社外取締役
- within them score across theme score 

ーーーーーーーー
モチベーション
サポート 分析ツール という位置づけ

クライアントからしたら詳細の計算伝わってないので 透明性が高いと言っても刺さらない
マテリアリティ という クライアント自身で時間かけてつけた重要課題を差し置いて 
mizuho scoreというのは不自然だからこそ マテリアリティから出発 するのが自然

みずほは 格付け 期間じゃないので水は スコアを上げると言っても直接はクライアントとに関係ないので 刺さらない。
（間接的に他の格付け評価も上がるけれど）
スコア 上がっても投資家がそれを見て 資本コスト下げるということもない

途中で みずほ スコアというマイルストーンを介するせることはいいにしても 
最初 最後はクライアントのメリットになるようなことで終わらせる

ーーーー

https://www.mdpi.com/2071-1050/12/20/8729

Do the same analysis but is company level data and ESG score with raw level items.
国⇨企業、SDGs→ESGスコアとして同様のPCA分析
全企業、同業限定など
相関の時間変化

Figure５
Applying PCA allows us to map trends, synergies and trade-offs at the level of goals for all SDGs while using all available information on indicators.


************************

https://www.sustainablebrands.jp/sbjlab/newscolumn/detail/1204199_2675.html

〔企業報告基準設定の主要団体によるマテリアリティの定義〕

GRI：企業の著しい経済的、環境的、社会的インパクトを反映するサステナビリティ事象、 あるいは利害関係者の評価と意思決定に実質的に影響を及ぼす事象。
(注)2020年6月に公表された共通スタンダード(100番台)の改定案では後半(下線部)が削除されているが、本稿執筆時点では確認できない。

CDSB：気候変動によるインパクトやその結果が、企業の財務状況や経営成績、あるいは戦略実行能力に著しいプラス・マイナスのインパクトを及ぼすと予想される事象。

IIRC：企業の短期・中期・長期の価値創造プロセスに実質的にインパクトを与えるプラス・マイナスの事象。財務要素と非財務要素の双方に適用される(企業とステークホルダーの価値創造をめざす)。

SASB：短期・中期・長期的な財務実績と企業価値の業種別評価に基づいて、ユーザーが行う投資・融資の決定に合理的にインパクトを与える可能性がある事象。

IFRS 財団 (IASB)：省略、誤記、または曖昧にした場合、企業の財務諸表に基づいて主要ユーザーが行う意思決定に合理的にインパクトを及ぼす可能性がある情報。

************************

https://andomitsunobu.net/?p=18984 


しかし、全上場企業でみれば、マテリアリティが特定できている企業は数割程度だし、その特定されているというマテリアリティも随分あやしく、事業の意思決定に貢献しないし価値創造を最大化するわけでもない、いわゆる“なんちゃってマテリアリティ”であったりします。

企業は営利組織なので、サステナビリティも最終的には財務視点で語られるべきです。今、社会的側面から語られる話も、ダイナミック・マテリアリティのロジックですが、社会の変化に合わせて、最終的に財務インパクトになるわけで。

リスク面のマテリアリティは、業界内でほぼ同じになるのに対して、機会（事業機会創出）としてのマテリアリティおよびそのKPIは、企業ごとに独自の成長モデルに貢献する項目になるはずです。IIRCでいう資本構造がすべての企業で異なるわけで、さすがに機会側面もマテリアリティまで競合と同じであればさすがにおかしいと気づくべきです。だから価値創造プロセスは、必ず企業独自のものになるのです。

さて、一応リスク管理という視点で、ダイナミック・マテリアリティにも言及しておきます。例えば、ある時点では「マテリアル（リスクおよび機会の重要項目）ではない」と考えられていたESG課題が、経済・環境・社会への企業のインパクトに関わるエビデンスを再検討した結果、マテリアルとなる可能性があります。これまでマテリアルとはならなかったESG課題が、時間の経過とともに、または急な社会変化により、企業の価値創造にとってマテリアルとなるのです。不確実性の高い時代だからこそ、今後のマテリアリティ特定には、ダイナミックマテリアリティの、動的な目標設定という視点がより求められるだろう。

************************
https://andomitsunobu.net/?p=19220 

マテリアリティ特定の要素も、GRI的な「ステークホルダーの重要度 × 企業の重要度」だけではなく、それに「リスクの影響度」「リスクの発生確率」「価値創造の貢献度」「財務インパクトの大きさ」とかが加わり、カオスな状態になっています。


************************

